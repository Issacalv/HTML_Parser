{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkOo2qrGTzPJ8+U8HDGkxt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r8RpulGRCr1Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "import os\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parent_folder = '/content/HTML_FOLDER' #This is where the HTML Files are stored in a folder\n",
        "output_csv = 'Google_search_result.csv' #Name the CSV file"
      ],
      "metadata": {
        "id": "EQe_7vYGc4pO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_links_titles_domains(parent_folder, output_csv, limit = None):\n",
        "    all_links = []\n",
        "    all_titles = []\n",
        "    all_domains = []\n",
        "    file_name = []\n",
        "    total_entries = 0\n",
        "\n",
        "    for html_file in sorted(os.listdir(parent_folder)):\n",
        "        if html_file.endswith('.html'):\n",
        "            with open(os.path.join(parent_folder, html_file), 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "            # All links fall under the 'a' class\n",
        "            # Extract links only that have an h3\n",
        "            #This removes the links that are not search links but may appear as bubbles on google\n",
        "            links_h3 = []\n",
        "\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                href = link['href']\n",
        "                if href.startswith('http'):\n",
        "                    if link.find('h3'):\n",
        "                        links_h3.append(link['href'])\n",
        "\n",
        "            # Extract titles\n",
        "            titles = []\n",
        "            for title in soup.find_all('h3'):\n",
        "                titles.append(title.get_text())\n",
        "\n",
        "            # Extract domains\n",
        "            domains = []\n",
        "            for link in links_h3:\n",
        "                domains.append(urlparse(link).netloc)\n",
        "\n",
        "            #Make all lists the same length by filling in missing values with \"NA\"\n",
        "            max_length = max(len(links_h3), len(titles), len(domains))\n",
        "\n",
        "            #Fill shorter lists with \"NA\"\n",
        "            links_h3 += ['NA'] * (max_length - len(links_h3))\n",
        "            titles += ['NA'] * (max_length - len(titles))\n",
        "            domains += ['NA'] * (max_length - len(domains))\n",
        "\n",
        "            #Add a max limit\n",
        "            for i in range(max_length):\n",
        "              if limit and total_entries >= limit:\n",
        "                break\n",
        "\n",
        "              # Append the extracted data to the overall lists\n",
        "              all_links.append(links_h3[i])\n",
        "              all_titles.append(titles[i])\n",
        "              all_domains.append(domains[i])\n",
        "              file_name.append(html_file)\n",
        "              total_entries = total_entries + 1\n",
        "\n",
        "            if limit and total_entries >= limit:\n",
        "              break\n",
        "\n",
        "\n",
        "\n",
        "    #Create a DataFrame\n",
        "    df = pd.DataFrame({\n",
        "       'Links': all_links,\n",
        "       'Titles': all_titles,\n",
        "       'Domains': all_domains,\n",
        "       'FileName': file_name\n",
        "    })\n",
        "\n",
        "    # Save to CSV or display the DataFrame\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Data saved to {output_csv}\")\n",
        "    print(df.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "tL1w94UESsBk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Arguments (3) are\n",
        "#Parent Folder\n",
        "#Output CSV file\n",
        "#How many links to extract\n",
        "extract_links_titles_domains(parent_folder, output_csv, limit = 60)\n"
      ],
      "metadata": {
        "id": "1XUBKORVc8y2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}