{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP163WIuuNmdCi5p3u1qyaH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ILwB7anqljPC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "import os\n",
        "import glob\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Current issues:\n",
        "Code is not robust enough to consider all possible file names\n",
        "The file names MUST end in a number before the extension.\n",
        "\n",
        "File name structure must follow:\n",
        "[FILE_NAME][NUMBER].[EXTENSION]\n",
        "'''"
      ],
      "metadata": {
        "id": "tr7xFL-WbJrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Data from HTML Websites"
      ],
      "metadata": {
        "id": "QxLGNBO905uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def natural_sort_key(s):\n",
        "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]"
      ],
      "metadata": {
        "id": "hW9u9751aH1j"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data(folder, output_file, limit = None):\n",
        "    all_links = []\n",
        "    all_titles = []\n",
        "    all_domains = []\n",
        "    file_name = []\n",
        "    total_entries = 0\n",
        "\n",
        "    output_file = output_file + folder.split('/')[-1] + '.xlsx'\n",
        "\n",
        "    for html_file in sorted(os.listdir(folder), key = natural_sort_key):\n",
        "        if html_file.endswith('.html'):\n",
        "            with open(os.path.join(folder, html_file), 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "            # All links fall under the 'a' class\n",
        "            # Extract links only that have an h3\n",
        "            #This removes the links that are not search links but may appear as bubbles on google\n",
        "            links_h3 = []\n",
        "\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                href = link['href']\n",
        "                if href.startswith('http'):\n",
        "                    if link.find('h3'):\n",
        "                        links_h3.append(link['href'])\n",
        "\n",
        "            # Extract titles\n",
        "            titles = []\n",
        "            for title in soup.find_all('h3'):\n",
        "                titles.append(title.get_text())\n",
        "\n",
        "            # Extract domains\n",
        "            domains = []\n",
        "            for link in links_h3:\n",
        "                domains.append(urlparse(link).netloc)\n",
        "\n",
        "            #Make all lists the same length by filling in missing values with \"NA\"\n",
        "            max_length = max(len(links_h3), len(titles), len(domains))\n",
        "\n",
        "            #Fill shorter lists with \"NA\"\n",
        "            links_h3 += ['NA'] * (max_length - len(links_h3))\n",
        "            titles += ['NA'] * (max_length - len(titles))\n",
        "            domains += ['NA'] * (max_length - len(domains))\n",
        "\n",
        "            #Add a max limit\n",
        "            for i in range(max_length):\n",
        "              if limit and total_entries >= limit:\n",
        "                break\n",
        "\n",
        "              # Append the extracted data to the overall lists\n",
        "              all_links.append(links_h3[i])\n",
        "              all_titles.append(titles[i])\n",
        "              all_domains.append(domains[i])\n",
        "              file_name.append(html_file)\n",
        "              total_entries = total_entries + 1\n",
        "\n",
        "            if limit and total_entries >= limit:\n",
        "              break\n",
        "\n",
        "\n",
        "\n",
        "    #Create a DataFrame\n",
        "    df = pd.DataFrame({\n",
        "       'Links': all_links,\n",
        "       'Titles': all_titles,\n",
        "       'Domains': all_domains,\n",
        "       'FileName': file_name\n",
        "    })\n",
        "\n",
        "    # Save to CSV or display the DataFrame\n",
        "\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "    print(f\"len(all_links): {len(all_links)}\")\n",
        "    print(f\"len(all_titles): {len(all_titles)}\")\n",
        "    print(f\"len(all_domains): {len(all_domains)}\")\n",
        "    print(df.head())\n",
        "\n",
        "    return all_links, all_titles, all_domains\n",
        "\n",
        "\n",
        "def compare_links(all_links_parent, all_links_new, viewLinks=False, export=False, export_name='new_links.xlsx'):\n",
        "    difference_list = []\n",
        "    for link in all_links_new:\n",
        "        if link not in all_links_parent:\n",
        "            difference_list.append(link)\n",
        "\n",
        "    if len(difference_list) == 0:\n",
        "        print(\"No new links found\")\n",
        "    else:\n",
        "        if viewLinks:\n",
        "            for link in difference_list:\n",
        "                print(link)\n",
        "        print(f\"New links found: {len(difference_list)}\")\n",
        "\n",
        "        if export:\n",
        "            if export_name.endswith('.txt'):\n",
        "                with open(export_name, 'w') as f:\n",
        "                    for link in difference_list:\n",
        "                        f.write(link + '\\n')\n",
        "                print(f\"New links exported to {export_name}\")\n",
        "\n",
        "            elif export_name.endswith('.xlsx'):\n",
        "                df_new_links = pd.DataFrame({'New Links': difference_list})\n",
        "                df_new_links.to_excel(export_name, index=False)\n",
        "                print(f\"New links exported to {export_name}\")\n",
        "\n",
        "            else:\n",
        "                export_name += '.xlsx'\n",
        "                df_new_links = pd.DataFrame({'New Links': difference_list})\n",
        "                df_new_links.to_excel(export_name, index=False)\n",
        "                print(f\"New links exported to {export_name}\")\n",
        "\n",
        "    return difference_list"
      ],
      "metadata": {
        "id": "GZ0z-E8MlmUY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def HTML_PIPELINE(parent_folder, output_file, compare_folder=None, viewLinks=False, export=False, export_name=\"new_links.xlsx\"):\n",
        "\n",
        "    all_links_parent, all_titles_parent, all_domains_parent = extract_data(parent_folder, output_file)\n",
        "\n",
        "    if compare_folder:\n",
        "        all_links_new, all_titles_new, all_domains_new = extract_data(compare_folder, output_file)\n",
        "        difference_list = compare_links(all_links_parent, all_links_new, viewLinks=viewLinks)\n",
        "\n",
        "        if export and difference_list:\n",
        "            if not export_name.endswith('.txt') and not export_name.endswith('.xlsx'):\n",
        "                export_name += '.xlsx'\n",
        "\n",
        "            if export_name.endswith('.txt'):\n",
        "                with open(export_name, 'w') as f:\n",
        "                    for link in difference_list:\n",
        "                        f.write(link + '\\n')\n",
        "                print(f\"New links exported to {export_name}\")\n",
        "\n",
        "            elif export_name.endswith('.xlsx'):\n",
        "                df_new_links = pd.DataFrame({'New Links': difference_list})\n",
        "                df_new_links.to_excel(export_name, index=False)\n",
        "                print(f\"New links exported to {export_name}\")\n",
        "    else:\n",
        "      print(\"Done\")\n",
        "\n"
      ],
      "metadata": {
        "id": "s6pG4zhdmAD4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Usage Example:\n",
        "\n",
        "HTML_PIPELINE(parent_folder, output_file, compare_folder, viewLinks=True, export=True, export_name=\"new_links.xlsx\")\n",
        "\n",
        "\n",
        "*   arg[0] = parent_folder #Mandatory | Folder containing all HTML files\n",
        "*   arg[1] = output_file #Mandatory | Name of the output file\n",
        "*   arg[2] = compare_folder #Optional | Compares new links to old links\n",
        "*   arg[3] = viewLinks #Optional | Prints out all new links in terminal\n",
        "*   arg[4] = export #Optional | Exports new links to a file\n",
        "*   arg[5] = export_name #Optional | Exports new links to a file\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r4Ls2uZa3x8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Usage Example:\n",
        "HTML_PIPELINE(parent_folder, output_file, compare_folder=None, viewLinks=False, export=False, export_name=\"new_links.xlsx\")\n",
        "\n",
        "arg[0] = parent_folder #Mandatory | Folder containing all HTML files\n",
        "arg[1] = output_file #Mandatory | Name of the output file\n",
        "arg[2] = compare_folder #Optional | Compares new links to old links\n",
        "arg[3] = viewLinks #Optional | Prints out all new links in terminal\n",
        "arg[4] = export #Optional | Exports new links to a file\n",
        "arg[5] = export_name #Optional | Exports new links to a file\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "3sEuyvVD2xug",
        "outputId": "934ddcb8-ee9c-4eb2-9eb6-db6c61cd154c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nUsage Example:\\nHTML_PIPELINE(parent_folder, output_file, compare_folder=None, viewLinks=False, export=False, export_name=\"new_links.xlsx\")\\n\\narg[0] = parent_folder #Mandatory | Folder containing all HTML files\\narg[1] = output_file #Mandatory | Name of the output file\\narg[2] = compare_folder #Optional | Compares new links to old links\\narg[3] = viewLinks #Optional | Prints out all new links in terminal\\narg[4] = export #Optional | Exports new links to a file\\narg[5] = export_name #Optional | Exports new links to a file\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HTML_PIPELINE(\"/content/HTML_files\", \"Google Searches_167\")"
      ],
      "metadata": {
        "id": "mPvuvvCz4QGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86307bdb-85b3-4d45-d6c0-c23deb9d5e65"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to Google Searches_167HTML_files.xlsx\n",
            "len(all_links): 177\n",
            "len(all_titles): 177\n",
            "len(all_domains): 177\n",
            "                                               Links  \\\n",
            "0  https://www.apexcoollabs.com/products/narwhals...   \n",
            "1  https://startupsmagazine.co.uk/article-portabl...   \n",
            "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...   \n",
            "3  https://www.apexcoollabs.com/industries/fire-s...   \n",
            "4  https://www.stucan-solutions.com/products-page...   \n",
            "\n",
            "                                              Titles  \\\n",
            "0  The Next Generation Narwhals: Palm Cooling Dev...   \n",
            "1  The portable body cooling device for heat illness   \n",
            "2  Use of an external-cooling device for the trea...   \n",
            "3        Heat Stress Mitigation for the Fire Service   \n",
            "4  CGX1 Core Temperature Cooling Device | Product...   \n",
            "\n",
            "                    Domains                                           FileName  \n",
            "0      www.apexcoollabs.com  _portable cooling device_ OR _cooling device_ ...  \n",
            "1    startupsmagazine.co.uk  _portable cooling device_ OR _cooling device_ ...  \n",
            "2      www.ncbi.nlm.nih.gov  _portable cooling device_ OR _cooling device_ ...  \n",
            "3      www.apexcoollabs.com  _portable cooling device_ OR _cooling device_ ...  \n",
            "4  www.stucan-solutions.com  _portable cooling device_ OR _cooling device_ ...  \n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qxmzfG1t69mz"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}